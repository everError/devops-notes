
## 🤖 LLM (Large Language Model)

로컬 환경에서 LLM을 운영하고 서비스하기 위한 내용을 다룹니다.

### 주요 내용

- **Ollama 서버 구축**: Docker Compose를 활용한 Ollama + Open WebUI 배포
- **GPU 최적화**: NVIDIA GPU를 활용한 LLM 추론 가속화
- **모델 관리**: 다양한 LLM 모델 다운로드, 설정 및 운영
- **시스템 요구사항**: CUDA, nvidia-container-toolkit 등 필수 구성 요소
- **성능 튜닝**: GPU 메모리 관리, Quantized 모델 활용
- **API 서비스**: Ollama API를 통한 LLM 서비스 제공 등

### 다루는 도구

- **Ollama**: 로컬 LLM 실행 서버
- **Open WebUI**: LLM을 위한 웹 기반 채팅 인터페이스
- **Docker & Docker Compose**: 컨테이너 기반 LLM 서비스 배포
- **NVIDIA GPU & CUDA**: GPU 가속을 위한 환경 설정