# Ollama + Open WebUI Docker Compose ì„¤ì • ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”
ì´ Docker Compose íŒŒì¼ì€ Ollama (ë¡œì»¬ LLM ì„œë²„)ì™€ Open WebUI (ì›¹ ì¸í„°í˜ì´ìŠ¤)ë¥¼ í•¨ê»˜ ì‹¤í–‰í•˜ëŠ” êµ¬ì„±ì…ë‹ˆë‹¤.

## ğŸ® ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ì‚¬í•­

#### 1. í•˜ë“œì›¨ì–´
- **NVIDIA GPU** (GeForce, Tesla, Quadro ë“±)

#### 2. ì†Œí”„íŠ¸ì›¨ì–´
Docker Compose íŒŒì¼ì—ì„œ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒì´ **ëª¨ë‘** í•„ìš”í•©ë‹ˆë‹¤:

1. **NVIDIA ë“œë¼ì´ë²„** (450.x ì´ìƒ ê¶Œì¥)
2. **Docker** (20.10 ì´ìƒ ê¶Œì¥)
3. **Docker Compose** (v2.x ê¶Œì¥)
4. **nvidia-container-toolkit** (nvidia-docker2)

### ì„¤ì¹˜ í™•ì¸ ë°©ë²•

```bash
# 1. GPU í™•ì¸
lspci | grep -i nvidia

# 2. NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# ì¶œë ¥ ì˜ˆì‹œ:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.54.03    Driver Version: 535.54.03    CUDA Version: 12.2   |
# +-----------------------------------------------------------------------------+

# 3. Docker í™•ì¸
docker --version
# ì¶œë ¥ ì˜ˆì‹œ: Docker version 24.0.5, build ced0996

# 4. Docker Compose í™•ì¸
docker compose version
# ì¶œë ¥ ì˜ˆì‹œ: Docker Compose version v2.20.2

# 5. nvidia-container-toolkit í™•ì¸ (ê°€ì¥ ì¤‘ìš”!)
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
# nvidia-smi ì¶œë ¥ì´ ë‚˜ì˜¤ë©´ ì •ìƒ
```

**ëª¨ë“  ëª…ë ¹ì–´ê°€ ì •ìƒ ì‘ë™í•˜ë©´ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œì…ë‹ˆë‹¤!**

### í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)

#### NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ (Ubuntu ì˜ˆì‹œ)
```bash
# ì¶”ì²œ ë“œë¼ì´ë²„ í™•ì¸
ubuntu-drivers devices

# ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜
sudo ubuntu-drivers autoinstall

# ë˜ëŠ” íŠ¹ì • ë²„ì „ ì„¤ì¹˜
sudo apt install nvidia-driver-535

# ì¬ë¶€íŒ…
sudo reboot

# ì„¤ì¹˜ í™•ì¸
nvidia-smi
```

#### Docker ì„¤ì¹˜
```bash
# Docker ê³µì‹ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# í˜„ì¬ ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER

# ì¬ë¡œê·¸ì¸ ë˜ëŠ”
newgrp docker

# í™•ì¸
docker --version
```

#### nvidia-container-toolkit ì„¤ì¹˜
```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Docker ì¬ì‹œì‘
sudo systemctl restart docker

# í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

## ğŸ—ï¸ ì„œë¹„ìŠ¤ êµ¬ì„±

### 1. Ollama ì„œë¹„ìŠ¤
ë¡œì»¬ì—ì„œ LLM ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ì„œë²„ì…ë‹ˆë‹¤.

**ì£¼ìš” ì„¤ì •:**
```yaml
services:
  ollama:
    image: ollama/ollama
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: ["gpu"]
            count: all
    ports:
      - "11434:11434"
    volumes:
      - /data/docker-data/ollama/data/ollama:/root/.ollama
    restart: always
```

**ì„¤ì • ì„¤ëª…:**
- **ì´ë¯¸ì§€**: `ollama/ollama` (CUDA ëŸ°íƒ€ì„ ë‚´ì¥)
- **GPU í• ë‹¹**: NVIDIA GPU ì „ì²´ ì‚¬ìš© (`count: all`)
  - `count: all` - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPU í• ë‹¹
  - `count: 1` - GPU 1ê°œë§Œ í• ë‹¹
  - `device_ids: ['0']` - íŠ¹ì • GPU ì„ íƒ (0ë²ˆ)
- **í¬íŠ¸**: `11434` (Ollama API ê¸°ë³¸ í¬íŠ¸)
- **ë³¼ë¥¨**: `/data/docker-data/ollama/data/ollama:/root/.ollama`
  - ëª¨ë¸ íŒŒì¼ê³¼ ì„¤ì •ì´ ì €ì¥ë˜ëŠ” ì˜êµ¬ ìŠ¤í† ë¦¬ì§€
  - **ì£¼ì˜**: í˜¸ìŠ¤íŠ¸ì— í•´ë‹¹ ë””ë ‰í† ë¦¬ê°€ ë¯¸ë¦¬ ì¡´ì¬í•´ì•¼ í•¨
- **ì¬ì‹œì‘ ì •ì±…**: `always` (ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ í•­ìƒ ì¬ì‹œì‘)

### 2. Open WebUI ì„œë¹„ìŠ¤
Ollamaë¥¼ ìœ„í•œ ì›¹ ê¸°ë°˜ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

**ì£¼ìš” ì„¤ì •:**
```yaml
openwebui:
  image: ghcr.io/open-webui/open-webui:latest
  restart: unless-stopped
  ports:
    - "3000:8080"
  environment:
    - OLLAMA_BASE_URL=http://ollama:11434
  volumes:
    - ./data/openwebui:/app/backend/data
  depends_on:
    - ollama
```

**ì„¤ì • ì„¤ëª…:**
- **ì´ë¯¸ì§€**: `ghcr.io/open-webui/open-webui:latest`
- **í¬íŠ¸**: `3000:8080` (í˜¸ìŠ¤íŠ¸ 3000 â†’ ì»¨í…Œì´ë„ˆ 8080)
- **ì ‘ì† URL**: `http://localhost:3000`
- **Ollama ì—°ê²°**: `OLLAMA_BASE_URL=http://ollama:11434`
  - Docker ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ì—ì„œ ì„œë¹„ìŠ¤ëª…(`ollama`)ìœ¼ë¡œ ì—°ê²°
- **ë³¼ë¥¨**: `./data/openwebui:/app/backend/data`
  - ì±„íŒ… ê¸°ë¡, ì‚¬ìš©ì ì„¤ì • ë“± ì €ì¥
  - í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
- **ì˜ì¡´ì„±**: `depends_on: ollama` (Ollamaê°€ ë¨¼ì € ì‹œì‘)
- **ì¬ì‹œì‘ ì •ì±…**: `unless-stopped` (ìˆ˜ë™ìœ¼ë¡œ ì¤‘ì§€í•˜ì§€ ì•ŠëŠ” í•œ ì¬ì‹œì‘)

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ì‚¬ì „ ì¤€ë¹„
```bash
# Ollama ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
sudo mkdir -p /data/docker-data/ollama/data/ollama
sudo chown -R $USER:$USER /data/docker-data/ollama

# Open WebUI ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„± (ìë™ ìƒì„±ë˜ì§€ë§Œ ë¯¸ë¦¬ ë§Œë“¤ì–´ë„ ë¨)
mkdir -p ./data/openwebui
```

### 2. GPU ìƒíƒœ í™•ì¸
```bash
# í˜¸ìŠ¤íŠ¸ì—ì„œ GPU í™•ì¸
nvidia-smi

# Dockerì—ì„œ GPU ì¸ì‹ í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

### 3. ì‹œì‘
```bash
# Docker Compose v2 (ê¶Œì¥)
docker compose up -d

# Docker Compose v1 (êµ¬ë²„ì „)
docker-compose up -d
```

### 4. ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
```bash
# ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ í™•ì¸
docker compose ps

# ì¶œë ¥ ì˜ˆì‹œ:
# NAME                STATUS              PORTS
# ollama              Up 2 minutes        0.0.0.0:11434->11434/tcp
# openwebui           Up 2 minutes        0.0.0.0:3000->8080/tcp
```

### 5. GPU ì‚¬ìš© í™•ì¸
```bash
# Ollama ì»¨í…Œì´ë„ˆì—ì„œ GPU í™•ì¸
docker compose exec ollama nvidia-smi

# í˜¸ìŠ¤íŠ¸ì—ì„œ ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# GPU ì‚¬ìš©ë¥  ìƒì„¸ ëª¨ë‹ˆí„°ë§
nvidia-smi dmon -s pucvmet
```

### 6. ë¡œê·¸ í™•ì¸
```bash
# ì „ì²´ ë¡œê·¸
docker compose logs -f

# Ollamaë§Œ
docker compose logs -f ollama

# Open WebUIë§Œ
docker compose logs -f openwebui

# ìµœê·¼ 100ì¤„ë§Œ ë³´ê¸°
docker compose logs --tail=100 ollama
```

### 7. ì ‘ì†
- **Open WebUI**: http://localhost:3000
  - ì²« ì ‘ì† ì‹œ ê³„ì • ìƒì„± í•„ìš”
- **Ollama API**: http://localhost:11434
  - API í™•ì¸: `curl http://localhost:11434/api/tags`

### 8. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‚¬ìš©
```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì˜ˆ: llama2 7B)
docker compose exec ollama ollama pull llama2

# ë‹¤ë¥¸ ëª¨ë¸ ì˜ˆì‹œ
docker compose exec ollama ollama pull mistral
docker compose exec ollama ollama pull codellama

# ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ í™•ì¸
docker compose exec ollama ollama list

# ì¶œë ¥ ì˜ˆì‹œ:
# NAME            ID              SIZE      MODIFIED
# llama2:latest   78e26419b446    3.8 GB    2 hours ago

# CLIì—ì„œ ëª¨ë¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
docker compose exec ollama ollama run llama2 "ì•ˆë…•í•˜ì„¸ìš”"

# ëª¨ë¸ ì‚­ì œ
docker compose exec ollama ollama rm llama2
```

### 9. ì¤‘ì§€ ë° ì¬ì‹œì‘
```bash
# ì¤‘ì§€ (ì»¨í…Œì´ë„ˆ ì‚­ì œ, ë°ì´í„°ëŠ” ìœ ì§€)
docker compose down

# ì¬ì‹œì‘
docker compose restart

# íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¬ì‹œì‘
docker compose restart ollama
```

### 10. ì™„ì „ ì‚­ì œ
```bash
# ì»¨í…Œì´ë„ˆ, ë„¤íŠ¸ì›Œí¬, ì´ë¯¸ì§€ ëª¨ë‘ ì‚­ì œ
docker compose down --rmi all --volumes

# ë°ì´í„° ë””ë ‰í† ë¦¬ë„ ì‚­ì œí•˜ë ¤ë©´
sudo rm -rf /data/docker-data/ollama
rm -rf ./data/openwebui
```

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
í”„ë¡œì íŠ¸/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ data/
    â””â”€â”€ openwebui/              # Open WebUI ë°ì´í„° (ìë™ ìƒì„±)
        â”œâ”€â”€ webui.db            # ì‚¬ìš©ì ê³„ì •, ì„¤ì •
        â””â”€â”€ uploads/            # ì—…ë¡œë“œëœ íŒŒì¼

ì™¸ë¶€ ë³¼ë¥¨:
/data/docker-data/ollama/
â””â”€â”€ data/
    â””â”€â”€ ollama/
        â”œâ”€â”€ models/             # ë‹¤ìš´ë¡œë“œëœ LLM ëª¨ë¸
        â””â”€â”€ .ollama/            # Ollama ì„¤ì •
```

## ğŸ”§ ì£¼ìš” í™˜ê²½ ë³€ìˆ˜ ë° ì˜µì…˜

### Ollama GPU ê´€ë ¨ ì„¤ì •

#### íŠ¹ì • GPU ì„ íƒ
```yaml
environment:
  # 0ë²ˆ GPUë§Œ ì‚¬ìš©
  - NVIDIA_VISIBLE_DEVICES=0
  
  # ì—¬ëŸ¬ GPU ì‚¬ìš©
  - NVIDIA_VISIBLE_DEVICES=0,1
  
  # ëª¨ë“  GPU ì‚¬ìš© (ê¸°ë³¸ê°’)
  - NVIDIA_VISIBLE_DEVICES=all
```

#### ë””ë²„ê·¸ ëª¨ë“œ
```yaml
environment:
  # ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥ (CUDA ì •ë³´, ëª¨ë¸ ë¡œë”© ê³¼ì • ë“±)
  - OLLAMA_DEBUG=2
```

#### ë‹¤ì¤‘ GPU í™˜ê²½ ì„¤ì • ì˜ˆì‹œ
```yaml
services:
  ollama:
    image: ollama/ollama
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0', '1']  # 0ë²ˆ, 1ë²ˆ GPUë§Œ ì‚¬ìš©
            capabilities: ["gpu"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1
```

### Open WebUI ê´€ë ¨ ì„¤ì •

#### Ollama ì—°ê²° ì„¤ì •
```yaml
environment:
  # ì™¸ë¶€ Ollama ì„œë²„ ì—°ê²°
  - OLLAMA_BASE_URL=http://ë‹¤ë¥¸ì„œë²„:11434
  
  # ì¸ì¦ì´ í•„ìš”í•œ ê²½ìš°
  - OLLAMA_API_KEY=your-api-key
```

#### í¬íŠ¸ ë³€ê²½
```yaml
ports:
  - "8080:8080"  # í¬íŠ¸ 8080ìœ¼ë¡œ ë³€ê²½
```

## ğŸ’¡ ì„±ëŠ¥ ìµœì í™” ë° íŒ

### 1. ëª¨ë¸ë³„ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
- **7B ëª¨ë¸** (llama2, mistral): ìµœì†Œ 8GB VRAM
- **13B ëª¨ë¸**: ìµœì†Œ 16GB VRAM
- **34B ëª¨ë¸**: ìµœì†Œ 24GB VRAM
- **70B+ ëª¨ë¸**: ìµœì†Œ 48GB VRAM (ë˜ëŠ” ë‹¤ì¤‘ GPU)

### 2. Quantized ëª¨ë¸ ì‚¬ìš©
ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©:
```bash
# 4-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ 1/4)
docker compose exec ollama ollama pull llama2:7b-q4_0

# 8-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ 1/2)
docker compose exec ollama ollama pull llama2:7b-q8_0
```

### 3. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
watch -n 1 'nvidia-smi --query-gpu=timestamp,name,memory.used,memory.total,utilization.gpu --format=csv'

# ë˜ëŠ”
nvidia-smi dmon -s mu
```

### 4. ë™ì‹œ ìš”ì²­ ì œí•œ
Open WebUIì—ì„œ ë§ì€ ì‚¬ìš©ìê°€ ë™ì‹œ ì‚¬ìš© ì‹œ:
```yaml
environment:
  - OLLAMA_NUM_PARALLEL=2  # ë™ì‹œ ì²˜ë¦¬ ìš”ì²­ ìˆ˜ ì œí•œ
```

### 5. ëª¨ë¸ ìºì‹±
```bash
# ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€ (ë¹ ë¥¸ ì‘ë‹µ)
# OllamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 5ë¶„ê°„ ë©”ëª¨ë¦¬ì— ìœ ì§€
# ì„¤ì • ë³€ê²½í•˜ë ¤ë©´:
environment:
  - OLLAMA_KEEP_ALIVE=10m  # 10ë¶„ê°„ ìœ ì§€
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **GPU í•„ìˆ˜**: OllamaëŠ” GPU ì—†ì´ë„ ì‹¤í–‰ë˜ì§€ë§Œ, CPU ì¶”ë¡ ì€ **ë§¤ìš° ëŠë¦½ë‹ˆë‹¤** (10-100ë°° ì°¨ì´)
2. **ë””ìŠ¤í¬ ê³µê°„**: ëª¨ë¸ í¬ê¸°ê°€ í¬ë¯€ë¡œ ì¶©ë¶„í•œ ê³µê°„ í•„ìš”
   - 7B ëª¨ë¸: ì•½ 4-8GB
   - 13B ëª¨ë¸: ì•½ 8-16GB
   - 70B ëª¨ë¸: ì•½ 40-80GB
3. **ë©”ëª¨ë¦¬**: ì‹œìŠ¤í…œ RAMë„ ì¶©ë¶„í•´ì•¼ í•¨ (ìµœì†Œ 16GB ê¶Œì¥)
4. **NVIDIA ë“œë¼ì´ë²„**: ë“œë¼ì´ë²„ ë²„ì „ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ìµœì‹  CUDAë¥¼ ì§€ì›í•˜ì§€ ëª»í•¨
5. **ë°©í™”ë²½**: í¬íŠ¸ 3000ê³¼ 11434ê°€ ì—´ë ¤ìˆì–´ì•¼ í•¨
6. **Docker Compose ë²„ì „**: v2 ì‚¬ìš© ê¶Œì¥ (`docker compose` ëª…ë ¹ì–´)
7. **ë³¼ë¥¨ ê¶Œí•œ**: `/data/docker-data/ollama` ë””ë ‰í† ë¦¬ì— ì“°ê¸° ê¶Œí•œ í•„ìš”

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. GPU ì¸ì‹ ì•ˆ ë¨
**ì¦ìƒ**: `nvidia-smi: command not found` ë˜ëŠ” GPU ì‚¬ìš© ì•ˆ ë¨

**í•´ê²°ë°©ë²•**:
```bash
# nvidia-container-toolkit í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

# ì‹¤íŒ¨ ì‹œ ì¬ì„¤ì¹˜
sudo apt-get install --reinstall nvidia-container-toolkit
sudo systemctl restart docker

# Docker ë°ëª¬ ì„¤ì • í™•ì¸
sudo cat /etc/docker/daemon.json

# ë‹¤ìŒ ë‚´ìš©ì´ ìˆì–´ì•¼ í•¨:
{
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}

# ì—†ë‹¤ë©´ ì¶”ê°€ í›„ Docker ì¬ì‹œì‘
sudo systemctl restart docker
```

### 2. "could not select device driver" ì˜¤ë¥˜
**ì¦ìƒ**: ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œ GPU ë“œë¼ì´ë²„ ì˜¤ë¥˜

**í•´ê²°ë°©ë²•**:
```bash
# nvidia-container-runtime ì„¤ì •
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# ë‹¤ì‹œ ì‹œë„
docker compose up -d
```

### 3. Ollama ì»¨í…Œì´ë„ˆê°€ ê³„ì† ì¬ì‹œì‘ë¨
**ì¦ìƒ**: `docker compose ps`ì—ì„œ Restarting ìƒíƒœ

**í•´ê²°ë°©ë²•**:
```bash
# ë¡œê·¸ í™•ì¸
docker compose logs ollama

# ë³¼ë¥¨ ê¶Œí•œ ë¬¸ì œì¼ ê°€ëŠ¥ì„±
sudo chown -R $USER:$USER /data/docker-data/ollama

# ë³¼ë¥¨ ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
ls -la /data/docker-data/ollama/data/ollama
```

### 4. Open WebUIì—ì„œ Ollama ì—°ê²° ì•ˆ ë¨
**ì¦ìƒ**: "Ollama connection failed" ì˜¤ë¥˜

**í•´ê²°ë°©ë²•**:
```bash
# Ollama ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker compose ps ollama

# Ollama API ì‘ë‹µ í™•ì¸
curl http://localhost:11434/api/tags

# Docker ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ì—ì„œ í™•ì¸
docker compose exec openwebui curl http://ollama:11434/api/tags

# ì‹¤íŒ¨ ì‹œ depends_on í™•ì¸ ë° ì¬ì‹œì‘
docker compose restart
```

### 5. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)
**ì¦ìƒ**: "CUDA out of memory" ì˜¤ë¥˜

**í•´ê²°ë°©ë²•**:
```bash
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
docker compose exec ollama ollama pull llama2:7b

# Quantized ëª¨ë¸ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
docker compose exec ollama ollama pull llama2:7b-q4_0

# ì‹¤í–‰ ì¤‘ì¸ ë‹¤ë¥¸ ëª¨ë¸ ì¢…ë£Œ
docker compose exec ollama ollama ps
# ì‚¬ìš© ì•ˆ í•˜ëŠ” ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
```

### 6. í¬íŠ¸ ì¶©ëŒ
**ì¦ìƒ**: "port is already allocated" ì˜¤ë¥˜

**í•´ê²°ë°©ë²•**:
```bash
# ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
sudo lsof -i :3000
sudo lsof -i :11434

# í¬íŠ¸ ë³€ê²½ (docker-compose.yml ìˆ˜ì •)
ports:
  - "3001:8080"  # 3000 â†’ 3001ë¡œ ë³€ê²½
  - "11435:11434"  # 11434 â†’ 11435ë¡œ ë³€ê²½
```

### 7. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
**ì¦ìƒ**: "error pulling model" ë˜ëŠ” ë‹¤ìš´ë¡œë“œ ì¤‘ë‹¨

**í•´ê²°ë°©ë²•**:
```bash
# ë„¤íŠ¸ì›Œí¬ í™•ì¸
docker compose exec ollama ping -c 3 ollama.ai

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h /data/docker-data/ollama

# ë‹¤ì‹œ ì‹œë„
docker compose exec ollama ollama pull llama2

# ë¶€ë¶„ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì‚­ì œ í›„ ì¬ì‹œë„
docker compose exec ollama rm -rf /root/.ollama/models/blobs/sha256-*
```

### 8. CUDA ë²„ì „ ë¶ˆì¼ì¹˜
**ì¦ìƒ**: CUDA ê´€ë ¨ ì˜¤ë¥˜ ë©”ì‹œì§€

**í•´ê²°ë°©ë²•**:
```bash
# í˜¸ìŠ¤íŠ¸ CUDA ë²„ì „ í™•ì¸
nvidia-smi | grep "CUDA Version"

# ë“œë¼ì´ë²„ê°€ ë„ˆë¬´ ì˜¤ë˜ëœ ê²½ìš° ì—…ë°ì´íŠ¸
sudo ubuntu-drivers autoinstall
sudo reboot
```

### 9. ê¶Œí•œ ë¬¸ì œ
**ì¦ìƒ**: Permission denied ì˜¤ë¥˜

**í•´ê²°ë°©ë²•**:
```bash
# Docker ê·¸ë£¹ì— ì‚¬ìš©ì ì¶”ê°€
sudo usermod -aG docker $USER
newgrp docker

# ë³¼ë¥¨ ë””ë ‰í† ë¦¬ ê¶Œí•œ ìˆ˜ì •
sudo chown -R $USER:$USER /data/docker-data/ollama
sudo chown -R $USER:$USER ./data/openwebui

# Docker ì†Œì¼“ ê¶Œí•œ í™•ì¸
sudo chmod 666 /var/run/docker.sock
```

### 10. ëŠë¦° ì¶”ë¡  ì†ë„
**ì¦ìƒ**: ì‘ë‹µ ìƒì„±ì´ ë„ˆë¬´ ëŠë¦¼

**í™•ì¸ì‚¬í•­**:
```bash
# GPUê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ í™•ì¸
nvidia-smi

# CPUë¡œ ì‹¤í–‰ë˜ê³  ìˆì„ ê°€ëŠ¥ì„±
docker compose logs ollama | grep -i "cuda\|gpu"

# GPU ì‚¬ìš©ë¥ ì´ ë‚®ë‹¤ë©´
# - ëª¨ë¸ì´ ë„ˆë¬´ ì‘ì„ ìˆ˜ ìˆìŒ
# - ë°°ì¹˜ í¬ê¸° ì¡°ì • í•„ìš”
# - ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ GPU ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìŒ
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **Ollama ê³µì‹ ë¬¸ì„œ**: https://github.com/ollama/ollama
- **Open WebUI ë¬¸ì„œ**: https://docs.openwebui.com
- **ì§€ì› ëª¨ë¸ ëª©ë¡**: https://ollama.com/library
- **NVIDIA Container Toolkit**: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/

## ğŸ“ ìœ ìš©í•œ ëª…ë ¹ì–´ ëª¨ìŒ

```bash
# === ëª¨ë¸ ê´€ë¦¬ ===
# ëª¨ë¸ ê²€ìƒ‰
docker compose exec ollama ollama list

# ëª¨ë¸ ì •ë³´ í™•ì¸
docker compose exec ollama ollama show llama2

# ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸
docker compose exec ollama ollama ps

# === ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ===
# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
docker stats

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
docker system df

# === ë¡œê·¸ ë° ë””ë²„ê¹… ===
# ì—ëŸ¬ ë¡œê·¸ë§Œ ë³´ê¸°
docker compose logs ollama | grep -i error

# íŠ¹ì • ì‹œê°„ ì´í›„ ë¡œê·¸
docker compose logs --since 30m ollama

# ì‹¤ì‹œê°„ ë¡œê·¸ (ë§ˆì§€ë§‰ 50ì¤„)
docker compose logs -f --tail=50

# === ë°±ì—… ë° ë³µêµ¬ ===
# ëª¨ë¸ ë°±ì—…
sudo tar -czf ollama-models-backup.tar.gz /data/docker-data/ollama

# Open WebUI ë°ì´í„° ë°±ì—…
tar -czf openwebui-backup.tar.gz ./data/openwebui

# ë³µêµ¬
sudo tar -xzf ollama-models-backup.tar.gz -C /
tar -xzf openwebui-backup.tar.gz
```